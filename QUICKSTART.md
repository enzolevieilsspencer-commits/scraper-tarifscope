# ‚ö° Quick Start Guide

Guide de d√©marrage rapide pour tester le scraper localement avant de d√©ployer.

## üì¶ Installation (5 minutes)

### 1. Cloner et installer

```bash
cd booking-scraper-project

# Cr√©er environnement virtuel
python -m venv venv

# Activer l'environnement
source venv/bin/activate  # Mac/Linux
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Installer Playwright
playwright install chromium
```

### 2. Configuration

Le fichier `.env` est d√©j√† configur√© avec vos cl√©s Supabase.

V√©rifier que tout est bon:
```bash
python test_setup.py
```

## üß™ Tests locaux

### Test 1: Scraper d'infos h√¥tel

```bash
python src/scrapers/hotel_info_scraper.py
```

Vous verrez le navigateur s'ouvrir et scraper un h√¥tel de test.

### Test 2: Scraper de prix

```bash
python src/scrapers/price_scraper.py
```

Cela va scraper les 30 prochains jours pour un h√¥tel test.

### Test 3: API (Scraper 1)

Terminal 1:
```bash
python src/api/server.py
```

Terminal 2:
```bash
curl -X POST http://localhost:8000/scrape-hotel \
  -H "Content-Type: application/json" \
  -d '{"url": "https://www.booking.com/hotel/fr/chateau-de-roussan.fr.html"}'
```

### Test 4: Scheduler automatique

```bash
# Ex√©cuter imm√©diatement (sans attendre les horaires)
python src/scheduler/cron_jobs.py --run-now

# Ou juste la session 1
python src/scheduler/cron_jobs.py --session 1
```

### Test 5: Run complet du scraper de prix

```bash
# Mode test (1 seul h√¥tel)
python src/scheduler/run_price_scraper.py --test

# Session 1 (3 premiers h√¥tels)
python src/scheduler/run_price_scraper.py --session 1

# Tous les h√¥tels
python src/scheduler/run_price_scraper.py
```

## üéØ Utilisation Production

### Option A: Scheduler automatique (recommand√©)

Lance le scheduler qui va scraper automatiquement 2x/jour:

```bash
python src/scheduler/cron_jobs.py
```

Laisser tourner en arri√®re-plan. Le scheduler va:
- Session 1: Entre 8h-11h (3 h√¥tels)
- Session 2: Entre 14h-17h (3 autres h√¥tels)
- Horaires randomis√©s chaque jour

### Option B: API pour ajouter des concurrents

Lance le serveur API:

```bash
python src/api/server.py
```

Endpoints disponibles:
- `GET /` - Info API
- `GET /health` - Health check
- `POST /scrape-hotel` - Ajouter un h√¥tel
- `POST /test-scrape` - Tester sans enregistrer

## üìä V√©rifier les r√©sultats

### Dans Supabase

1. Aller sur https://supabase.com
2. Votre projet ‚Üí Table Editor
3. Regarder les tables:
   - `hotels` - Les h√¥tels ajout√©s
   - `rate_snapshots` - Les prix scrap√©s
   - `scraper_logs` - Les logs d'ex√©cution

### Requ√™tes SQL utiles

```sql
-- Voir tous les h√¥tels actifs
SELECT * FROM hotels WHERE "isMonitored" = true;

-- Voir les derniers prix scrap√©s
SELECT h.name, rs."dateCheckin", rs.price, rs.available
FROM rate_snapshots rs
JOIN hotels h ON h.id = rs."hotelId"
ORDER BY rs."scrapedAt" DESC
LIMIT 100;

-- Voir les logs des derni√®res ex√©cutions
SELECT * FROM scraper_logs
ORDER BY "startedAt" DESC
LIMIT 10;
```

## üöÄ Pr√™t pour le d√©ploiement ?

1. ‚úÖ Tests locaux OK
2. ‚úÖ Donn√©es bien enregistr√©es dans Supabase
3. ‚úÖ Aucune erreur dans les logs

‚Üí Suivre [DEPLOYMENT.md](./DEPLOYMENT.md) pour d√©ployer sur Railway

## üÜò Probl√®mes courants

### "ModuleNotFoundError"
```bash
pip install -r requirements.txt
```

### "playwright executable doesn't exist"
```bash
playwright install chromium
```

### "Connection refused" (Supabase)
V√©rifier les cl√©s dans `.env`

### Le scraper ne trouve pas les prix
- Booking.com change parfois sa structure HTML
- V√©rifier les s√©lecteurs dans `price_scraper.py`
- Augmenter les timeouts

### Trop de requ√™tes bloqu√©es
Augmenter les d√©lais dans `.env`:
```
MIN_DELAY_SECONDS=60
MAX_DELAY_SECONDS=120
```

## üìö Prochaines √©tapes

1. **Local OK ?** ‚Üí D√©ployer sur Railway ([DEPLOYMENT.md](./DEPLOYMENT.md))
2. **Besoin d'int√©grer avec Next.js ?** ‚Üí Voir [NEXTJS_INTEGRATION.md](./NEXTJS_INTEGRATION.md)
3. **Personnaliser ?** ‚Üí Modifier les fichiers dans `src/`

## üí° Conseils

- **Headless mode**: Mettre `HEADLESS_MODE=true` en production
- **Logs**: Toujours v√©rifier `scraper_logs` apr√®s ex√©cution
- **Monitoring**: Configurer des alertes Railway pour les √©checs
- **Backup**: Exporter r√©guli√®rement la base Supabase

Bon scraping ! üéâ
